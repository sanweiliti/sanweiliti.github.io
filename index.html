<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Siwei Zhang </title>
  
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="stylesheet" type="text/css" href="styles_responsive.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">

  <script src="https://cdn.jsdelivr.net/npm/prismjs-bibtex@2.0.1/prism-bibtex.min.js"></script>
  <link rel="icon" type="image/png" href="images/icon.png">

</head>

<body>
    <div class="container_onecolumn">
        <div class="container">
          <div class="item text">
              <p>
                <name>Siwei Zhang (张四维)</name>
              </p>
              <p>I am a PhD student at <a href="https://vlg.inf.ethz.ch/">Computer Vision and Learning Group </a>(VLG),
                <a href="https://ethz.ch/en.html">ETH Zürich</a>, supervised by <a href="https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html">Siyu Tang</a>.
                Prior to this, I obtained my Master degree (2020) in Electrical Engineering and Information Technology, ETH Zürich,
                and Bachelor degree (2017) in Automation, <a href="https://www.tsinghua.edu.cn/en/" target="_blank" rel="noopener">Tsinghua University</a>.
              </p>
              <p>My research focuses on human-scene interaction learning, human motion modelling and egocentric human understanding, particularly with the 3D scenes.
              </p>
              <center>
              <p>
                <a href="mailto:siwei.zhang@inf.ethz.ch"><i class="fa fa-paper-plane"></i>&nbsp&nbspEmail</a></a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=q7JVOrQAAAAJ&hl=en"><i class="ai ai-google-scholar ai-fw" style="font-size: 1.3em;position: relative; top:0.1em;margin-left: -0.3em;"></i>Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/SiweiZhang13"><i class="fa fa-twitter"></i>&nbsp&nbspTwitter</a> &nbsp/&nbsp
                <a href="https://github.com/sanweiliti/"><i class="fa fa-github"></i>&nbsp&nbspGithub</a>
              </p>
                </center>
            </div>

<!--            <div class="container">-->
<!--  <img src="imgs_main/me.jpg" alt="图片居中">-->
<!--</div>-->
          <div class="circular image">
<!--              <img src="imgs_main/me.jpg" alt="Avatar">-->
<!--              <div class="item image">-->
<!--              <img src="imgs_main/me.jpg" class="rounded-circle profile mr-3 mb-sm-1 float-sm-left" />-->
              <img alt="profile photo" src="imgs_main/me.jpg">
            </div>
          </div>
          
          
      <div class="item text2">
                <p>
                    <heading>Publications</heading>
                </p>
            </div>


        <hr>

      <div class="container2">
            <div class="item image2">
              <img src='imgs_main/egohmr.jpg' width=180; height="auto">
            </div>
            <div class="item text2">
              <a href="https://sanweiliti.github.io/egohmr/egohmr.html">
                <papertitle>Probabilistic Human Mesh Recovery in 3D Scenes from Egocentric Views</papertitle></font>
                </a>
                <br>
                <strong>Siwei Zhang</strong>,
                <a href="https://qianlim.github.io/">Qianli Ma</a>,
                <a href="https://yz-cnsdqz.github.io/">Yan Zhang</a>,
                <a href="https://sadegh-aa.github.io/">Sadegh Aliakbarian</a>,
                <a href="http://www.cs.bath.ac.uk/~dpc/">Darren Cosker</a>,
                <a href=https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html>Siyu Tang</a>
                <br>
                <em>ICCV</em>, 2023 <font color="red"><strong>Oral Presentation</strong></font>
                <br>

                <div class="links">
                    <a href="https://neuralbodies.github.io/arah/">Project page</a>
                </div>
                <div class="links">
                    <a href="https://github.com/sanweiliti/EgoHMR">Code</a>
                </div>
                <div class="links">
                    <a href="https://arxiv.org/abs/2304.06024">arXiv</a>
                </div>
                <div class="links">
                    <a href="https://youtu.be/K6m0BmfMG-E?si=Eum1jcg2DcqSsbcn">Video</a>
                </div>
<!--                <a href="https://sanweiliti.github.io/egohmr/egohmr.html">Project Page</a> /-->
<!--                <a href="https://github.com/sanweiliti/EgoHMR">Code</a> /-->
<!--                <a href="https://arxiv.org/abs/2304.06024">arXiv</a> /-->
<!--                <a href="https://youtu.be/K6m0BmfMG-E?si=Eum1jcg2DcqSsbcn">Video</a> /-->
                <button id="bib_button" class="links" onclick="showBib('EgoHMR_bib')">BibTex</button>
                <div id='EgoHMR_bib' hidden>
  <pre><code>@inproceedings{Egobody:ECCV:2022,
  title = {Probabilistic Human Mesh Recovery in 3D Scenes from Egocentric Views},
  author = {Siwei Zhang, Qianli Ma, Yan Zhang, Sadegh Aliakbarian, Darren Cosker, Siyu Tang},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages = {7989--8000},
  month = oct,
  year = {2023}
}</code></pre>
  </div>

          <p></p>
          <p>
            Generative human mesh recovery for images with body occlusion and truncations: 
            scene-conditioned diffusion model + collision-guided sampling = accurate pose estimation on observed
            body parts and plausible generation of unobserved parts.
          </p>
        </div>
      </div>

      <hr>

      <div class="container2">
        <div class="item image2">
          <img src='imgs_main/POV_surgery.jpg' width=180; height="auto">
        </div>
        <div class="item text2">
          <a href="https://batfacewayne.github.io/POV_Surgery_io/">
            <papertitle>POV-Surgery: A Dataset for Egocentric Hand and Tool Pose Estimation During Surgical Activities</papertitle></font>
          </a>
          <br>
          <a href=https://ch.linkedin.com/in/rui-wang-47b0361a5/>Rui Wang</a>*,
          <a href=https://pdz.ethz.ch/the-group/people/Sophokles.html/>Sophokles Ktistakis</a>*,
          <strong>Siwei Zhang</strong>,
          <a href=https://pdz.ethz.ch/the-group/people/meboldt.html>Mirko Meboldt</a>,
          <a href=https://pdz.ethz.ch/the-group/people/lohmeyer.html>Quentin Lohmeyer</a>
          <br>
          <em>MICCAI</em>, 2023 <font color="red"><strong>Oral presentation</strong></font>
          <br>

          <div class="links">
              <a href="https://batfacewayne.github.io/POV_Surgery_io/">Project Page</a>
          </div>
            <div class="links">
              <a href="https://github.com/BatFaceWayne/POV_Surgery.git">Code</a>
          </div>
            <div class="links">
              <a href="https://drive.google.com/drive/folders/1nSDig2cEHscCPgG10-VcSW3Q1zKge4tP?usp=drive_link">Dataset
          </div>
            <div class="links">
              <a href="https://arxiv.org/abs/2307.10387">arXiv</a>
          </div>

<!--          <a href="https://batfacewayne.github.io/POV_Surgery_io/">Project Page</a> /-->
<!--          <a href="https://github.com/BatFaceWayne/POV_Surgery.git">Code</a> /-->
<!--          <a href="https://drive.google.com/drive/folders/1nSDig2cEHscCPgG10-VcSW3Q1zKge4tP?usp=drive_link">Dataset</a> /-->
<!--          <a href="https://arxiv.org/abs/2307.10387">arXiv</a> /-->
          <button id="bib_button" class="links", onclick="showBib('POV_bib')">BibTex</button>
          <div id='POV_bib' hidden>
          <pre><code>@inproceedings{wang2023pov,
  title={POV-Surgery: A Dataset for Egocentric Hand and Tool Pose Estimation During Surgical Activities},
  author={Wang, Rui and Ktistakis, Sophokles and Zhang, Siwei and Meboldt, Mirko and Lohmeyer, Quentin},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={440--450},
  year={2023}
}</code></pre>
        </div>        
      <p></p>
      <p>
        POV-Surgery is a synthetic egocentric dataset focusing on hand pose estimation with different surgical gloves and orthopedic surgical instruments,
      featuring RGB-D videos with annotations for activieis, 3D/2D hand-object pose, and 2D hand-object segmentation masks. </p>
    </div>
  </div>

      <hr>

      <div class="container2">
<div class="item image2">
  <img src='imgs_main/EgoBody4.png' id='egobody_image' width=180; height="auto">
</div>
<div class="item text2">
  <a href="https://sanweiliti.github.io/egobody/egobody.html">
    <papertitle>EgoBody: Human Body Shape and Motion of Interacting People from Head-Mounted Devices
    </papertitle>
  </a>
  <br>
  <strong>Siwei Zhang</strong>,
  <a href="https://qianlim.github.io/">Qianli Ma</a>,
  <a href="https://yz-cnsdqz.github.io/">Yan Zhang</a>,
  Zhiyin Qian,
  <a href="https://taeinkwon.com/">Taein Kwon</a>,
  <a href="https://people.inf.ethz.ch/pomarc/">Marc Pollefeys</a>,
  <a href="https://fbogo.github.io/">Federica Bogo</a>,
  <a href="https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html">Siyu
    Tang</a>
  <br>
  <em>ECCV</em>, 2022
  <br>
    <div class="links">
        <a href="https://sanweiliti.github.io/egobody/egobody.html">Project Page</a>
    </div>
    <div class="links">
        <a href="https://github.com/sanweiliti/EgoBody">Code</a>
    </div>
    <div class="links">
        <a href="https://egobody.ethz.ch/">Dataset</a>
    </div>
    <div class="links">
        <a href="https://arxiv.org/abs/2112.07642">arXiv</a>
    </div>
    <div class="links">
        <a href="https://youtu.be/yA7BM7zWAKM">Video</a>
    </div>

<!--  <a href="https://sanweiliti.github.io/egobody/egobody.html">Project Page</a>-->
<!--  / <a href="https://github.com/sanweiliti/EgoBody">Code</a>-->
<!--  / <a href="https://egobody.ethz.ch/">Dataset</a>-->
<!--  / <a href="https://arxiv.org/abs/2112.07642">arXiv</a>-->
<!--  / <a href="https://youtu.be/yA7BM7zWAKM">Video</a>-->
  <button id="bib_button" class="links", onclick="showBib('EboBody_bib')">BibTex</button>
  <div id='EboBody_bib' hidden>
  <pre><code>@inproceedings{Egobody:ECCV:2022,
  title = {{EgoBody}: Human Body Shape and Motion of Interacting People from Head-Mounted Devices},
  author = {Zhang, Siwei and Ma, Qianli and Zhang, Yan and Qian, Zhiyin and Kwon, Taein and Pollefeys, Marc and Bogo, Federica and Tang, Siyu},
  booktitle = {European Conference on Computer Vision (ECCV)},
  month = oct,
  year = {2022}
}</code></pre>
  </div>        
  <p></p>
  <p>
  A large-scale dataset of accurate 3D body shape, pose and motion of humans interacting in 3D scenes, with multi-modal streams from third-person and egocentric views, captured by Azure Kinects and a HoloLens2.
  </p>
  </div>
</div>

      <hr>

      <div class="container2">
<div class="item image2">
  <img src='imgs_main/saga.jpg' width=180; height="auto">
</div>
<div class="item text2">
  <a href="https://jiahaoplus.github.io/SAGA/saga.html">
    <font color=#1772d0>  <papertitle>SAGA: Stochastic Whole-Body Grasping with Contact</papertitle></font>
  </a>
  <br>
  <a href=https://wuyan01.github.io/>Yan Wu</a>*,
  <a href=https://jiahaoplus.github.io/>Jiahao Wang</a>*,
  <a href=https://yz-cnsdqz.github.io/>Yan Zhang</a>,
  <strong>Siwei Zhang</strong>,
  <a href=https://ait.ethz.ch/people/hilliges>Otmar Hilliges</a>,
    <a href=https://www.yf.io/>Fisher Yu</a>,
  <a href=https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html>Siyu Tang</a>
  <br>
  <em>ECCV</em>, 2022
  <br>
    <div class="links">
      <a href="https://jiahaoplus.github.io/SAGA/saga.html">Project Page</a>
    </div>
        <div class="links">
      <a href="https://github.com/JiahaoPlus/SAGA">Code</a>
    </div>
        <div class="links">
      <a href="https://arxiv.org/abs/2112.10103">arXiv</a>
    </div>
        <div class="links">
      <a href="https://www.youtube.com/watch?v=pX25sHNCvVE">Video</a>
    </div>
<!--  <a href="https://jiahaoplus.github.io/SAGA/saga.html">Project Page</a> /-->
<!--  <a href="https://github.com/JiahaoPlus/SAGA">Code</a> /-->
<!--  <a href="https://arxiv.org/abs/2112.10103">arXiv</a> /-->
<!--  <a href="https://www.youtube.com/watch?v=pX25sHNCvVE">Video</a> /-->
  <button id="bib_button" class="links", onclick="showBib('SAGA_bib')">BibTex</button>
  <div id='SAGA_bib' hidden>
    <pre><code>@inproceedings{wu2022saga,
   title = {SAGA: Stochastic Whole-Body Grasping with Contact},
   author = {Wu, Yan and Wang, Jiahao and Zhang, Yan and Zhang, Siwei and Hilliges, Otmar and Yu, Fisher and Tang, Siyu},
   booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
   year = {2022}
}</code></pre>
  </div>

  <p></p>
  <p>Starting from an arbitrary initial pose, SAGA generates diverse and natural whole-body human motions to approach and grasp a target object in 3D space.</p>
</div>
</div>

      <hr>

      <div class="container2">
<div class="item image2">
  <img src='imgs_main/lemo.jpg' width=180; height="auto">
</div>
<div class="item text2">
  <a href="https://sanweiliti.github.io/LEMO/LEMO.html">
    <papertitle>Learning Motion Priors for 4D Human Body Capture in 3D Scenes</papertitle>
  </a>
  <br>

  <strong>Siwei Zhang</strong>,
  <a href=https://yz-cnsdqz.github.io/>Yan Zhang</a>,
  <a href=https://fbogo.github.io/>Federica Bogo</a>,
  <a href=https://people.inf.ethz.ch/pomarc/>Marc Pollefeys</a>,
  <a href=https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html>Siyu Tang</a>
  <br>
  <em>ICCV</em>, 2021 <font color="red"><strong>Oral Presentation</strong></font>
  <br>
    <div class="links">
        <a href="https://sanweiliti.github.io/LEMO/LEMO.html">Project Page</a>
    </div>
        <div class="links">
        <a href="https://github.com/sanweiliti/LEMO">Code</a>
    </div>
        <div class="links">
        <a href="https://arxiv.org/abs/2108.10399">arXiv</a>
    </div>
        <div class="links">
        <a href="https://www.youtube.com/watch?v=AT14Y975-dc">Video</a>
    </div>
<!--  <a href="https://sanweiliti.github.io/LEMO/LEMO.html">Project Page</a> /-->
<!--  <a href="https://github.com/sanweiliti/LEMO">Code</a> /-->
<!--  <a href="https://arxiv.org/abs/2108.10399">arXiv</a> /-->
<!--  <a href="https://www.youtube.com/watch?v=AT14Y975-dc">Video</a> /-->
  
  <button id="bib_button" class="links", onclick="showBib('LEMO_bib')">BibTex</button>
  <div id='LEMO_bib' hidden>
  <pre><code>@inproceedings{Zhang:ICCV:2021,
   title = {Learning Motion Priors for 4D Human Body Capture in 3D Scenes},
   author = {Zhang, Siwei and Zhang, Yan and Bogo, Federica and Pollefeys Marc and Tang, Siyu},
   booktitle = {International Conference on Computer Vision (ICCV)},
   month = oct,
   year = {2021}
}</code></pre>
  </div>
  <p></p>
  <p>LEMO learns motion priors from a larger scale mocap dataset and proposes a multi-stage optimization
    pipeline to enable 3D motion reconstruction in complex 3D scenes.</p>
</div>
</div>

      <hr>

      <div class="container2">
<div class="item image2">
  <img src='imgs_main/noisyFER.jpg' width=180; height="auto">
</div>
<div class="item text2">
  <a href="https://openaccess.thecvf.com/content/WACV2021/papers/Zhang_Facial_Emotion_Recognition_With_Noisy_Multi-Task_Annotations_WACV_2021_paper.pdf">
    <papertitle>Facial Emotion Recognition with Noisy Multi-task Annotations</papertitle>
  </a>
  <br>
  <strong>Siwei Zhang</strong>,
  <a href=https://zhiwu-huang.github.io/>Zhiwu Huang</a>,
  <a href=https://people.ee.ethz.ch/~paudeld/>Danda Pani Paudel</a>,
  <a href=https://ee.ethz.ch/the-department/faculty/professors/person-detail.OTAyMzM=.TGlzdC80MTEsMTA1ODA0MjU5.html/>Luc Van Gool</a>
  <br>
  <em>WACV</em>, 2021
  <br>
    <div class="links">
        <a href="https://github.com/sanweiliti/noisyFER">Code</a>
    </div>
        <div class="links">
        <a href="https://openaccess.thecvf.com/content/WACV2021/papers/Zhang_Facial_Emotion_Recognition_With_Noisy_Multi-Task_Annotations_WACV_2021_paper.pdf">PDF</a>
    </div>
<!--  <a href="https://github.com/sanweiliti/noisyFER">Code</a> /-->
<!--  <a href="https://openaccess.thecvf.com/content/WACV2021/papers/Zhang_Facial_Emotion_Recognition_With_Noisy_Multi-Task_Annotations_WACV_2021_paper.pdf">PDF</a> /-->
  <button id="bib_button" class="links", onclick="showBib('NoisyFER_bib')">BibTex</button>
  <div id='NoisyFER_bib' hidden>
  <pre><code>@inproceedings{zhang2021facial,
   title = {Facial Emotion Recognition with Noisy Multi-task Annotations},
   author = {Zhang, Siwei and Huang, Zhiwu and Paudel, Danda Pani and Gool, Luc Van},
   booktitle = {Winter Conference on Applications of Computer Vision (WACV)},
    month={jan},
    year = {2021}
}</code></pre>
</div>
  
  <p></p>
  <p>To reduce human labelling effort on multi-task labels, we introduce a new problem of
    facial emotion recognition with noisy multi-task annotations.</p>
</div>
</div>

      <hr>

      <div class="container2">
  <div class="item image2">
    <img src='imgs_main/NAS.jpg' width=180; height="auto">
  </div>
  <div class="item text2">
    <a href="https://arxiv.org/abs/2007.16112">
      <papertitle>Neural architecture search as sparse supernet</papertitle>
    </a>
    <br>
    <a href=https://wuyan01.github.io/>Yan Wu</a>*,
    Aoming Liu*,
    <a href=https://zhiwu-huang.github.io/>Zhiwu Huang</a>,
    <strong>Siwei Zhang</strong>,
    <a href=https://ee.ethz.ch/the-department/faculty/professors/person-detail.OTAyMzM=.TGlzdC80MTEsMTA1ODA0MjU5.html/>Luc Van Gool</a>
    <br>
    <em>AAAI</em>, 2021
    <br>
      <div class="links">
          <a href="https://arxiv.org/abs/2007.16112">arXiv</a>
      </div>
<!--    <a href="https://arxiv.org/abs/2007.16112">arXiv</a> /-->
    <button id="bib_button" class="links", onclick="showBib('NAS_bib')">BibTex</button>
    <div id='NAS_bib' hidden>
    <pre><code>@inproceedings{wu2021neural,
   title = {Neural architecture search as sparse supernet},
   author = {Wu, Yan and Liu, Aoming and Huang, Zhiwu and Zhang, Siwei and Van Gool, Luc},
   booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
    volume={35},
   number={12},
  pages={10379--10387},
   year = {2021}
}</code></pre>
    </div>
    <p></p>
    <p>We model the NAS problem as a sparse supernet using a new continuous architecture representation
      with a mixture of sparsity constraints.</p>
    </div>
  </div>

      <hr>

      <div class="container2">
  <div class="item image2">
    <img src='imgs_main/PLACE.jpeg' width=180; height="auto">
  </div>
  <div class="item text2">
    <a href="https://sanweiliti.github.io/PLACE/PLACE.html">
      <papertitle>PLACE: Proximity Learning of Articulation and Contact in 3D Environments</papertitle>
    </a>
    <br>

    <strong>Siwei Zhang</strong>,
    <a href="https://yz-cnsdqz.github.io/">Yan Zhang</a>,
    <a href="https://qianlim.github.io/">Qianli Ma</a>,
    <a href=https://ps.is.tuebingen.mpg.de/person/black/>Michael J. Black</a>, 
    <a href=https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html>Siyu Tang</a>
    <br>
    <em>3DV</em>, 2020
    <br>
      <div class="links">
        <a href="https://sanweiliti.github.io/PLACE/PLACE.html">Project Page</a>
      </div>
            <div class="links">
        <a href="https://github.com/sanweiliti/PLACE">Code</a>
      </div>
            <div class="links">
        <a href="https://arxiv.org/abs/2008.05570">arXiv</a>
      </div>
            <div class="links">
        <a href="https://youtu.be/zJ1hbtMHGrw">Video</a>
      </div>
<!--    <a href="https://sanweiliti.github.io/PLACE/PLACE.html">Project Page</a> /-->
<!--    <a href="https://github.com/sanweiliti/PLACE">Code</a> / -->
<!--    <a href="https://arxiv.org/abs/2008.05570">arXiv</a> /-->
<!--    <a href="https://youtu.be/zJ1hbtMHGrw">Video</a> / -->
    <button id="bib_button" class="links", onclick="showBib('PLACE_bib')">BibTex</button>
    <div id='PLACE_bib' hidden>
    <pre><code>@inproceedings{PLACE:3DV:2020,
  title = {{PLACE}: Proximity Learning of Articulation and Contact in {3D} Environments},
  author = {Zhang, Siwei and Zhang, Yan and Ma, Qianli and Black, Michael J. and Tang, Siyu},
  booktitle = {International Conference on 3D Vision (3DV)},
  pages = {642--651},
  month = nov,
  year = {2020}
}</code></pre>
    </div>      
    <p></p>
    <p>An explicit representation for 3D person-scene contact relations that enables
      automated synthesis of realistic humans posed naturally in a given scene.</p>
  </div>
  </div>

      <hr>

      <div class="container2">
    <div class="item image2">
      <img src='imgs_main/facereenact.jpg' width=180; height="auto">
    </div>
    <div class="item text2">
      <a href="https://arxiv.org/abs/1908.03251">
        <papertitle>One-shot face reenactment</papertitle>
      </a>
      <br>
      Yunxuan Zhang,
      <strong>Siwei Zhang</strong>,
      Yue He,
      <a href=https://scholar.google.com/citations?user=F5rVlz0AAAAJ&hl=en>Cheng Li</a>,
      <a href=https://www.mmlab-ntu.com/person/ccloy/>Chen Change Loy</a>,
      <a href=https://liuziwei7.github.io/>Ziwei Liu</a>
      <br>
      <em>BMVC</em>, 2019 <font color="red"><strong>Spotlight Presentation</strong></font>
      <br>
        <div class="links">
             <a href="https://github.com/bj80heyue/One_Shot_Face_Reenactment">Code</a>
        </div>
        <div class="links">
            <a href="https://arxiv.org/abs/1908.03251">arXiv</a>
        </div>

<!--      <a href="https://github.com/bj80heyue/One_Shot_Face_Reenactment">Code</a> /-->
<!--      <a href="https://arxiv.org/abs/1908.03251">arXiv</a> /-->
      <button id="bib_button" class="links", onclick="showBib('facereenact_bib')">BibTex</button>
      <div id='facereenact_bib' hidden>
      <pre><code>@inproceedings{zhang2019one,
   title = {One-shot Face Reenactment},
   author = {Zhang, Yunxuan and Zhang, Siwei and He, Yue and Li, Cheng and Loy, Chen Change and Liu, Ziwei},
   booktitle = {BMVC},
   month = September,
   year = {2019}
}</code></pre>
      </div>

      <p></p>
      <p>We propose a novel one-shot face reenactment learning system, that is able to disentangle and
        compose appearance and shape information for effective modeling.</p>
</div>
    </div>


      <div class="item text2">
        <p>
          <heading>Awards</heading>
        </p>
        <p> <a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/2023-europe">Qualcomm Innovative Fellowship Europe 2023 </a> </p>
        </div>

        </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;">
                   Template adapted from <a href="https://jonbarron.info/">Jon Barron</a>'s and <a href="https://qianlim.github.io/">Qianli Ma</a>'s websites.
                </p>
              </td>
            </tr>
          </tbody></table>

<script type="text/javascript" src="show_bib.js"></script>
</body>
</html>
