<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Siwei Zhang </title>
  
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="keywords" content="Siwei Zhang; 张四维">
  <meta name="author" content="Siwei Zhang">

  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="stylesheet" type="text/css" href="styles_responsive.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
  <link href="https://fonts.googleapis.com/css2?family=Candara&display=swap" rel="stylesheet">

  <script src="https://cdn.jsdelivr.net/npm/prismjs-bibtex@2.0.1/prism-bibtex.min.js"></script>
<!--  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>&#128507</text></svg>">-->

<!--  <link rel="icon" type="image/png" href="images/icon.png">-->

</head>

<body>
<br>
    <div class="container_onecolumn">
        <div class="container">
          <div class="item text">
              <p>
                <name>Siwei Zhang (张四维)</name>
              </p>
              <p>I am a PhD student at <a href="https://vlg.inf.ethz.ch/">Computer Vision and Learning Group </a>(VLG),
                <a href="https://ethz.ch/en.html">ETH Zürich</a>, supervised by <a href="https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html">Siyu Tang</a>.
                Prior to this, I obtained my Master degree (2020) in Electrical Engineering and Information Technology, ETH Zürich,
                and Bachelor degree (2017) in Automation, <a href="https://www.tsinghua.edu.cn/en/" target="_blank" rel="noopener">Tsinghua University</a>.
              </p>
              <p>My research focuses on human motion modelling and human-scene interaction learning.
              </p>
<!--              <p>My research focuses on human-scene interaction learning, human motion modelling and egocentric human understanding, particularly with the 3D scenes.-->
<!--              </p>-->
              <center>
              <p>
                <a href="mailto:siwei.zhang@inf.ethz.ch"><i class="fa fa-paper-plane"></i>&nbsp&nbspEmail</a></a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=q7JVOrQAAAAJ&hl=en"><i class="ai ai-google-scholar ai-fw" style="font-size: 1.3em;position: relative; top:0.1em;margin-left: -0.3em;"></i>Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/SiweiZhang13"><i class="fa fa-twitter"></i>&nbsp&nbspTwitter</a> &nbsp/&nbsp
                <a href="https://github.com/sanweiliti/"><i class="fa fa-github"></i>&nbsp&nbspGithub</a>
              </p>
                </center>

<!--              <p></p>-->
<!--              <p><br><font color="red"><strong>I am actively looking for full-time positions and postdoc positions.</strong></font> Feel free to reach out to me! </p>-->
            </div>


        <center>
            <div class="circular image">
                  <img alt="profile photo" src="imgs_main/me.jpg">
            </div>
        </center>

        </div>



      <div class="item text2">
                <p>
                    <heading>Publications</heading>
                </p>
            </div>


        <hr>



      <div class="container2">
            <div class="item image2">
              <img src='imgs_main/rohm.jpg' width=180; height="auto">
            </div>
            <div class="item text2">
              <a href="https://sanweiliti.github.io/ROHM/ROHM.html">
                <papertitle>RoHM: Robust Human Motion Reconstruction via Diffusion</papertitle>
                </a>
                <br>
                <strong>Siwei Zhang</strong>,
                <a href="https://virtualhumans.mpi-inf.mpg.de/people/Bhatnagar.html"><authorname>Bharat Lal Bhatnagar</authorname></a>,
                <a href="https://web.cs.ucla.edu/~yuanluxu/"><authorname>Yuanlu Xu</authorname></a>,
                <a href="https://alex-winkler.com/"><authorname>Alexander Winkler</authorname></a>,
                <a href="https://scholar.google.com/citations?user=V_qztpEAAAAJ&hl=en"><authorname>Petr Kadlecek</authorname></a>,
                <a href="https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html"><authorname>Siyu Tang</authorname></a>,
                <a href="https://fbogo.github.io/"><authorname>Federica Bogo</authorname></a>
                <br>
                <em>CVPR</em>, 2024 <font color="red"><strong>Oral Presentation</strong></font>
                <br>

                <div class="box">
                    <a href="https://sanweiliti.github.io/ROHM/ROHM.html"><i class="fa fa-external-link"></i> Project page</a>
                </div>
                <div class="box">
                    <a href="https://github.com/sanweiliti/RoHM"> <i class="fa fa-github"></i> Code</a>
                </div>
                <div class="box">
                    <a href="https://arxiv.org/abs/2401.08570"><i class="fa fa-file-pdf-o"></i> arXiv</a>
                </div>
<!--                <div class="box">-->
<!--                    <a href="https://youtu.be/K6m0BmfMG-E?si=Eum1jcg2DcqSsbcn"><i class="fa fa-video-camera"></i> Video</a>-->
<!--                </div>-->
                <button id="bib_button" class="box" onclick="showBib('RoHM_bib')"> <i class="fa fa-quote-left"></i> BibTex</button>
                <div id='RoHM_bib' hidden>
  <pre><code>@inproceedings{zhang2024rohm,
  title={RoHM: Robust Human Motion Reconstruction via Diffusion},
  author={Zhang, Siwei and Bhatnagar, Bharat Lal and Xu, Yuanlu and Winkler, Alexander and Kadlecek, Petr and Tang, Siyu and Bogo, Federica},
  booktitle={CVPR},
  year={2024}
}</code></pre>
  </div>

          <p></p>
          <p>
            Conditioned on noisy and occluded input data, RoHM reconstructs complete, plausible motions in consistent global coordinates.
          </p>
        </div>
      </div>

      <div class="container2">
            <div class="item image2">
              <img src='imgs_main/egogen.jpg' width=180; height="auto">
            </div>
            <div class="item text2">
              <a href="https://ego-gen.github.io/">
                <papertitle>EgoGen: An Egocentric Synthetic Data Generator</papertitle>
                </a>
                <br>
                <a href="https://vlg.inf.ethz.ch/team/Gen-Li.html"><authorname>Gen Li</authorname></a>,
                <a href="https://vlg.inf.ethz.ch/team/Kaifeng-Zhao.html"><authorname>Kaifeng Zhao</authorname></a>,
                <strong>Siwei Zhang</strong>,
                <a href="https://vlg.inf.ethz.ch/team/Xiaozhong-Lyu.html"><authorname>Xiaozhong Lyu</authorname></a>,
                <a href="https://dusmanu.com/"><authorname>Mihai Dusmanu</authorname></a>,
                <a href="https://yz-cnsdqz.github.io/"><authorname>Yan Zhang</authorname></a>,
                <a href="https://cvg.ethz.ch/team/Prof-Dr-Marc-Pollefeys"><authorname>Marc Pollefeys</authorname></a>,
                <a href="https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html"><authorname>Siyu Tang</authorname></a>
                <br>
                <em>CVPR</em>, 2024 <font color="red"><strong>Oral Presentation</strong></font>
                <br>

                <div class="box">
                    <a href="https://ego-gen.github.io/"><i class="fa fa-external-link"></i> Project page</a>
                </div>
                <div class="box">
                    <a href="https://github.com/ligengen/EgoGen"> <i class="fa fa-github"></i> Code</a>
                </div>
                <div class="box">
                    <a href="https://arxiv.org/abs/2401.08739"><i class="fa fa-file-pdf-o"></i> arXiv</a>
                </div>
                <button id="bib_button" class="box" onclick="showBib('egogen_bib')"> <i class="fa fa-quote-left"></i> BibTex</button>
                <div id='egogen_bib' hidden>
  <pre><code>@inproceedings{li2024egogen,
  title={EgoGen: An Egocentric Synthetic Data Generator},
  author={Li, Gen and Zhao, Kaifeng and Zhang, Siwei and Lyu, Xiaozhong and Dusmanu, Mihai and Zhang, Yan and Pollefeys, Marc and Tang, Siyu},
  booktitle={CVPR},
  year={2024}
}</code></pre>
  </div>

          <p></p>
          <p>
            EgoGen is new synthetic data generator that can produce accurate and rich ground-truth training data for egocentric perception tasks.
          </p>
        </div>
      </div>


      <div class="container2">
            <div class="item image2">
              <img src='imgs_main/egohmr.jpg' width=180; height="auto">
            </div>
            <div class="item text2">
              <a href="https://sanweiliti.github.io/egohmr/egohmr.html">
                <papertitle>Probabilistic Human Mesh Recovery in 3D Scenes from Egocentric Views</papertitle>
                </a>
                <br>
                <strong>Siwei Zhang</strong>,
                <a href="https://qianlim.github.io/"><authorname>Qianli Ma</authorname></a>,
                <a href="https://yz-cnsdqz.github.io/"><authorname>Yan Zhang</authorname></a>,
                <a href="https://sadegh-aa.github.io/"><authorname>Sadegh Aliakbarian</authorname></a>,
                <a href="http://www.cs.bath.ac.uk/~dpc/"><authorname>Darren Cosker</authorname></a>,
                <a href=https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html><authorname>Siyu Tang</authorname></a>
                <br>
                <em>ICCV</em>, 2023 <font color="red"><strong>Oral Presentation</strong></font>
                <br>


                <div class="box">
                    <a href="https://sanweiliti.github.io/egohmr/egohmr.html"><i class="fa fa-external-link"></i> Project page</a>
                </div>
                <div class="box">
                    <a href="https://github.com/sanweiliti/EgoHMR"> <i class="fa fa-github"></i> Code</a>
                </div>
                <div class="box">
                    <a href="https://arxiv.org/abs/2304.06024"><i class="fa fa-file-pdf-o"></i> arXiv</a>
                </div>
                <div class="box">
                    <a href="https://youtu.be/K6m0BmfMG-E?si=Eum1jcg2DcqSsbcn"><i class="fa fa-video-camera"></i> Video</a>
                </div>
<!--                <a href="https://sanweiliti.github.io/egohmr/egohmr.html">Project Page</a> /-->
<!--                <a href="https://github.com/sanweiliti/EgoHMR">Code</a> /-->
<!--                <a href="https://arxiv.org/abs/2304.06024">arXiv</a> /-->
<!--                <a href="https://youtu.be/K6m0BmfMG-E?si=Eum1jcg2DcqSsbcn">Video</a> /-->
                <button id="bib_button" class="box" onclick="showBib('EgoHMR_bib')"> <i class="fa fa-quote-left"></i> BibTex</button>
                <div id='EgoHMR_bib' hidden>
  <pre><code>@inproceedings{zhang2023probabilistic,
  title = {Probabilistic Human Mesh Recovery in 3D Scenes from Egocentric Views},
  author = {Siwei Zhang, Qianli Ma, Yan Zhang, Sadegh Aliakbarian, Darren Cosker, Siyu Tang},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages = {7989--8000},
  month = oct,
  year = {2023}
}</code></pre>
  </div>

          <p></p>
          <p>
            Generative human mesh recovery for images with body occlusion and truncations:
            scene-conditioned diffusion model + collision-guided sampling = accurate pose estimation on observed
            body parts and plausible generation of unobserved parts.
          </p>
        </div>
      </div>

      <hr>

      <div class="container2">
        <div class="item image2">
          <img src='imgs_main/POV_surgery.jpg' width=180; height="auto">
        </div>
        <div class="item text2">
          <a href="https://batfacewayne.github.io/POV_Surgery_io/">
            <papertitle>POV-Surgery: A Dataset for Egocentric Hand and Tool Pose Estimation During Surgical Activities</papertitle></font>
          </a>
          <br>
          <a href=https://ch.linkedin.com/in/rui-wang-47b0361a5/><authorname>Rui Wang</authorname></a>*,
          <a href=https://pdz.ethz.ch/the-group/people/Sophokles.html/><authorname>Sophokles Ktistakis</authorname></a>*,
          <strong>Siwei Zhang</strong>,
          <a href=https://pdz.ethz.ch/the-group/people/meboldt.html><authorname>Mirko Meboldt</authorname></a>,
          <a href=https://pdz.ethz.ch/the-group/people/lohmeyer.html><authorname>Quentin Lohmeyer</authorname></a>
          <br>
          <em>MICCAI</em>, 2023 <font color="red"><strong>Oral presentation</strong></font>
          <br>

          <div class="box">
              <a href="https://batfacewayne.github.io/POV_Surgery_io/"><i class="fa fa-external-link"></i> Project Page</a>
          </div>
            <div class="box">
              <a href="https://github.com/BatFaceWayne/POV_Surgery.git"><i class="fa fa-github"></i> Code</a>
          </div>
            <div class="box">
              <a href="https://drive.google.com/drive/folders/1nSDig2cEHscCPgG10-VcSW3Q1zKge4tP?usp=drive_link"><i class="fa fa-database"></i> Dataset
          </div>
            <div class="box">
              <a href="https://arxiv.org/abs/2307.10387"><i class="fa fa-file-pdf-o"></i> arXiv</a>
          </div>

<!--          <a href="https://batfacewayne.github.io/POV_Surgery_io/">Project Page</a> /-->
<!--          <a href="https://github.com/BatFaceWayne/POV_Surgery.git">Code</a> /-->
<!--          <a href="https://drive.google.com/drive/folders/1nSDig2cEHscCPgG10-VcSW3Q1zKge4tP?usp=drive_link">Dataset</a> /-->
<!--          <a href="https://arxiv.org/abs/2307.10387">arXiv</a> /-->
          <button id="bib_button" class="box", onclick="showBib('POV_bib')"><i class="fa fa-quote-left"></i> BibTex</button>
          <div id='POV_bib' hidden>
          <pre><code>@inproceedings{wang2023pov,
  title={POV-Surgery: A Dataset for Egocentric Hand and Tool Pose Estimation During Surgical Activities},
  author={Wang, Rui and Ktistakis, Sophokles and Zhang, Siwei and Meboldt, Mirko and Lohmeyer, Quentin},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={440--450},
  year={2023}
}</code></pre>
        </div>
      <p></p>
      <p>
        POV-Surgery is a synthetic egocentric dataset focusing on hand pose estimation with different surgical gloves and orthopedic surgical instruments,
      featuring RGB-D videos with annotations for activieis, 3D/2D hand-object pose, and 2D hand-object segmentation masks. </p>
    </div>
  </div>

      <hr>

      <div class="container2">
<div class="item image2">
  <img src='imgs_main/EgoBody4.png' id='egobody_image' width=180; height="auto">
</div>
<div class="item text2">
  <a href="https://sanweiliti.github.io/egobody/egobody.html">
    <papertitle>EgoBody: Human Body Shape and Motion of Interacting People from Head-Mounted Devices
    </papertitle>
  </a>
  <br>
  <strong>Siwei Zhang</strong>,
  <a href="https://qianlim.github.io/"><authorname>Qianli Ma</authorname></a>,
  <a href="https://yz-cnsdqz.github.io/"><authorname>Yan Zhang</authorname></a>,
  <authorname-nolink>Zhiyin Qian</authorname-nolink>,
  <a href="https://taeinkwon.com/"><authorname>Taein Kwon</authorname></a>,
  <a href="https://people.inf.ethz.ch/pomarc/"><authorname>Marc Pollefeys</a>,
  <a href="https://fbogo.github.io/"><authorname>Federica Bogo</authorname></a>,
  <a href="https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html"><authorname>Siyu Tang</authorname></a>
  <br>
  <em>ECCV</em>, 2022
  <br>
    <div class="box">
        <a href="https://sanweiliti.github.io/egobody/egobody.html"><i class="fa fa-external-link"></i> Project Page</a>
    </div>
    <div class="box">
        <a href="https://github.com/sanweiliti/EgoBody"><i class="fa fa-github"></i> Code</a>
    </div>
    <div class="box">
        <a href="https://egobody.ethz.ch/"><i class="fa fa-database"></i> Dataset</a>
    </div>
    <div class="box">
        <a href="https://arxiv.org/abs/2112.07642"><i class="fa fa-file-pdf-o"></i> arXiv</a>
    </div>
    <div class="box">
        <a href="https://youtu.be/yA7BM7zWAKM"><i class="fa fa-video-camera"></i> Video</a>
    </div>

<!--  <a href="https://sanweiliti.github.io/egobody/egobody.html">Project Page</a>-->
<!--  / <a href="https://github.com/sanweiliti/EgoBody">Code</a>-->
<!--  / <a href="https://egobody.ethz.ch/">Dataset</a>-->
<!--  / <a href="https://arxiv.org/abs/2112.07642">arXiv</a>-->
<!--  / <a href="https://youtu.be/yA7BM7zWAKM">Video</a>-->
  <button id="bib_button" class="box", onclick="showBib('EboBody_bib')"><i class="fa fa-quote-left"></i> BibTex</button>
  <div id='EboBody_bib' hidden>
  <pre><code>@inproceedings{zhang2022egobody,
  title={Egobody: Human body shape and motion of interacting people from head-mounted devices},
  author={Zhang, Siwei and Ma, Qianli and Zhang, Yan and Qian, Zhiyin and Kwon, Taein and Pollefeys, Marc and Bogo, Federica and Tang, Siyu},
  booktitle={European Conference on Computer Vision},
  pages={180--200},
  year={2022},
  organization={Springer}
}</code></pre>
  </div>
  <p></p>
  <p>
  A large-scale dataset of accurate 3D body shape, pose and motion of humans interacting in 3D scenes, with multi-modal streams from third-person and egocentric views, captured by Azure Kinects and a HoloLens2.
  </p>
  </div>
</div>

      <hr>

      <div class="container2">
<div class="item image2">
  <img src='imgs_main/saga.jpg' width=180; height="auto">
</div>
<div class="item text2">
  <a href="https://jiahaoplus.github.io/SAGA/saga.html">
    <font color=#1772d0>  <papertitle>SAGA: Stochastic Whole-Body Grasping with Contact</papertitle></font>
  </a>
  <br>
  <a href=https://wuyan01.github.io/><authorname>Yan Wu</authorname></a>*,
  <a href=https://jiahaoplus.github.io/><authorname>Jiahao Wang</authorname></a>*,
  <a href=https://yz-cnsdqz.github.io/><authorname>Yan Zhang</authorname></a>,
  <strong>Siwei Zhang</strong>,
  <a href=https://ait.ethz.ch/people/hilliges><authorname>Otmar Hilliges</authorname></a>,
    <a href=https://www.yf.io/><authorname>Fisher Yu</authorname></a>,
  <a href=https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html><authorname>Siyu Tang</authorname></a>
  <br>
    (* denotes equal contribution)
    <br>
  <em>ECCV</em>, 2022
  <br>
    <div class="box">
      <a href="https://jiahaoplus.github.io/SAGA/saga.html"><i class="fa fa-external-link"></i> Project Page</a>
    </div>
        <div class="box">
      <a href="https://github.com/JiahaoPlus/SAGA"><i class="fa fa-github"></i> Code</a>
    </div>
        <div class="box">
      <a href="https://arxiv.org/abs/2112.10103"><i class="fa fa-file-pdf-o"></i> arXiv</a>
    </div>
        <div class="box">
      <a href="https://www.youtube.com/watch?v=pX25sHNCvVE"><i class="fa fa-video-camera"></i> Video</a>
    </div>
<!--  <a href="https://jiahaoplus.github.io/SAGA/saga.html">Project Page</a> /-->
<!--  <a href="https://github.com/JiahaoPlus/SAGA">Code</a> /-->
<!--  <a href="https://arxiv.org/abs/2112.10103">arXiv</a> /-->
<!--  <a href="https://www.youtube.com/watch?v=pX25sHNCvVE">Video</a> /-->
  <button id="bib_button" class="box", onclick="showBib('SAGA_bib')"><i class="fa fa-quote-left"></i> BibTex</button>
  <div id='SAGA_bib' hidden>
    <pre><code>@inproceedings{wu2022saga,
  title={Saga: Stochastic whole-body grasping with contact},
  author={Wu, Yan and Wang, Jiahao and Zhang, Yan and Zhang, Siwei and Hilliges, Otmar and Yu, Fisher and Tang, Siyu},
  booktitle={European Conference on Computer Vision},
  pages={257--274},
  year={2022},
  organization={Springer}
}</code></pre>
  </div>

  <p></p>
  <p>Starting from an arbitrary initial pose, SAGA generates diverse and natural whole-body human motions to approach and grasp a target object in 3D space.</p>
</div>
</div>

      <hr>

      <div class="container2">
<div class="item image2">
  <img src='imgs_main/lemo.jpg' width=180; height="auto">
</div>
<div class="item text2">
  <a href="https://sanweiliti.github.io/LEMO/LEMO.html">
    <papertitle>Learning Motion Priors for 4D Human Body Capture in 3D Scenes</papertitle>
  </a>
  <br>

  <strong>Siwei Zhang</strong>,
  <a href=https://yz-cnsdqz.github.io/><authorname>Yan Zhang</authorname></a>,
  <a href=https://fbogo.github.io/><authorname>Federica Bogo</authorname></a>,
  <a href=https://people.inf.ethz.ch/pomarc/><authorname>Marc Pollefeys</authorname></a>,
  <a href=https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html><authorname>Siyu Tang</authorname></a>
  <br>
  <em>ICCV</em>, 2021 <font color="red"><strong>Oral Presentation</strong></font>
  <br>
    <div class="box">
        <a href="https://sanweiliti.github.io/LEMO/LEMO.html"><i class="fa fa-external-link"></i> Project Page</a>
    </div>
        <div class="box">
        <a href="https://github.com/sanweiliti/LEMO"><i class="fa fa-github"></i> Code</a>
    </div>
        <div class="box">
        <a href="https://arxiv.org/abs/2108.10399"><i class="fa fa-file-pdf-o"></i> arXiv</a>
    </div>
        <div class="box">
        <a href="https://www.youtube.com/watch?v=AT14Y975-dc"><i class="fa fa-video-camera"></i> Video</a>
    </div>
<!--  <a href="https://sanweiliti.github.io/LEMO/LEMO.html">Project Page</a> /-->
<!--  <a href="https://github.com/sanweiliti/LEMO">Code</a> /-->
<!--  <a href="https://arxiv.org/abs/2108.10399">arXiv</a> /-->
<!--  <a href="https://www.youtube.com/watch?v=AT14Y975-dc">Video</a> /-->

  <button id="bib_button" class="box", onclick="showBib('LEMO_bib')"><i class="fa fa-quote-left"></i> BibTex</button>
  <div id='LEMO_bib' hidden>
  <pre><code>@inproceedings{zhang2021learning,
  title={Learning motion priors for 4d human body capture in 3d scenes},
  author={Zhang, Siwei and Zhang, Yan and Bogo, Federica and Pollefeys, Marc and Tang, Siyu},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={11343--11353},
  year={2021}
}</code></pre>
  </div>
  <p></p>
  <p>LEMO learns motion priors from a larger scale mocap dataset and proposes a multi-stage optimization
    pipeline to enable 3D motion reconstruction in complex 3D scenes.</p>
</div>
</div>

      <hr>

      <div class="container2">
  <div class="item image2">
    <img src='imgs_main/PLACE.jpeg' width=180; height="auto">
  </div>
  <div class="item text2">
    <a href="https://sanweiliti.github.io/PLACE/PLACE.html">
      <papertitle>PLACE: Proximity Learning of Articulation and Contact in 3D Environments</papertitle>
    </a>
    <br>

    <strong>Siwei Zhang</strong>,
    <a href="https://yz-cnsdqz.github.io/"><authorname>Yan Zhang</authorname></a>,
    <a href="https://qianlim.github.io/"><authorname>Qianli Ma</authorname></a>,
    <a href=https://ps.is.tuebingen.mpg.de/person/black/><authorname>Michael J. Black</authorname></a>,
    <a href=https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html><authorname>Siyu Tang</authorname></a>
    <br>
    <em>3DV</em>, 2020
    <br>
      <div class="box">
        <a href="https://sanweiliti.github.io/PLACE/PLACE.html"><i class="fa fa-external-link"></i> Project Page</a>
      </div>
            <div class="box">
        <a href="https://github.com/sanweiliti/PLACE"><i class="fa fa-github"></i> Code</a>
      </div>
            <div class="box">
        <a href="https://arxiv.org/abs/2008.05570"><i class="fa fa-file-pdf-o"></i> arXiv</a>
      </div>
            <div class="box">
        <a href="https://youtu.be/zJ1hbtMHGrw"><i class="fa fa-video-camera"></i> Video</a>
      </div>
<!--    <a href="https://sanweiliti.github.io/PLACE/PLACE.html">Project Page</a> /-->
<!--    <a href="https://github.com/sanweiliti/PLACE">Code</a> / -->
<!--    <a href="https://arxiv.org/abs/2008.05570">arXiv</a> /-->
<!--    <a href="https://youtu.be/zJ1hbtMHGrw">Video</a> / -->
    <button id="bib_button" class="box", onclick="showBib('PLACE_bib')"><i class="fa fa-quote-left"></i> BibTex</button>
    <div id='PLACE_bib' hidden>
    <pre><code>@inproceedings{zhang2020place,
  title={PLACE: Proximity learning of articulation and contact in 3D environments},
  author={Zhang, Siwei and Zhang, Yan and Ma, Qianli and Black, Michael J and Tang, Siyu},
  booktitle={2020 International Conference on 3D Vision (3DV)},
  pages={642--651},
  year={2020},
  organization={IEEE}
}</code></pre>
    </div>
    <p></p>
    <p>An explicit representation for 3D person-scene contact relations that enables
      automated synthesis of realistic humans posed naturally in a given scene.</p>
  </div>
  </div>

      <hr>

      <div class="container2">
<div class="item image2">
  <img src='imgs_main/noisyFER.jpg' width=180; height="auto">
</div>
<div class="item text2">
  <a href="https://openaccess.thecvf.com/content/WACV2021/papers/Zhang_Facial_Emotion_Recognition_With_Noisy_Multi-Task_Annotations_WACV_2021_paper.pdf">
    <papertitle>Facial Emotion Recognition with Noisy Multi-task Annotations</papertitle>
  </a>
  <br>
  <strong>Siwei Zhang</strong>,
  <a href=https://zhiwu-huang.github.io/><authorname>Zhiwu Huang</authorname></a>,
  <a href=https://people.ee.ethz.ch/~paudeld/><authorname>Danda Pani Paudel</authorname></a>,
  <a href=https://ee.ethz.ch/the-department/faculty/professors/person-detail.OTAyMzM=.TGlzdC80MTEsMTA1ODA0MjU5.html/><authorname>Luc Van Gool</authorname></a>
  <br>
  <em>WACV</em>, 2021
  <br>
    <div class="box">
        <a href="https://github.com/sanweiliti/noisyFER"><i class="fa fa-github"></i> Code</a>
    </div>
        <div class="box">
        <a href="https://openaccess.thecvf.com/content/WACV2021/papers/Zhang_Facial_Emotion_Recognition_With_Noisy_Multi-Task_Annotations_WACV_2021_paper.pdf"><i class="fa fa-file-pdf-o"></i> PDF</a>
    </div>
<!--  <a href="https://github.com/sanweiliti/noisyFER">Code</a> /-->
<!--  <a href="https://openaccess.thecvf.com/content/WACV2021/papers/Zhang_Facial_Emotion_Recognition_With_Noisy_Multi-Task_Annotations_WACV_2021_paper.pdf">PDF</a> /-->
  <button id="bib_button" class="box", onclick="showBib('NoisyFER_bib')"><i class="fa fa-quote-left"></i> BibTex</button>
  <div id='NoisyFER_bib' hidden>
  <pre><code>@inproceedings{zhang2021facial,
   title = {Facial Emotion Recognition with Noisy Multi-task Annotations},
   author = {Zhang, Siwei and Huang, Zhiwu and Paudel, Danda Pani and Gool, Luc Van},
   booktitle = {Winter Conference on Applications of Computer Vision (WACV)},
    month={jan},
    year = {2021}
}</code></pre>
</div>

  <p></p>
  <p>To reduce human labelling effort on multi-task labels, we introduce a new problem of
    facial emotion recognition with noisy multi-task annotations.</p>
</div>
</div>

      <hr>

      <div class="container2">
  <div class="item image2">
    <img src='imgs_main/NAS.jpg' width=180; height="auto">
  </div>
  <div class="item text2">
    <a href="https://arxiv.org/abs/2007.16112">
      <papertitle>Neural Architecture Search as Sparse Supernet</papertitle>
    </a>
    <br>
    <a href=https://wuyan01.github.io/><authorname>Yan Wu</authorname></a>*,
    <authorname-nolink>Aoming Liu</authorname-nolink>*,
    <a href=https://zhiwu-huang.github.io/><authorname>Zhiwu Huang</authorname></a>,
    <strong>Siwei Zhang</strong>,
    <a href=https://ee.ethz.ch/the-department/faculty/professors/person-detail.OTAyMzM=.TGlzdC80MTEsMTA1ODA0MjU5.html/><authorname>Luc Van Gool</authorname></a>
    <br>
      (* denotes equal contribution)
      <br>
    <em>AAAI</em>, 2021
    <br>
      <div class="box">
          <a href="https://arxiv.org/abs/2007.16112"><i class="fa fa-file-pdf-o"></i> arXiv</a>
      </div>
<!--    <a href="https://arxiv.org/abs/2007.16112">arXiv</a> /-->
    <button id="bib_button" class="box", onclick="showBib('NAS_bib')"><i class="fa fa-quote-left"></i> BibTex</button>
    <div id='NAS_bib' hidden>
    <pre><code>@inproceedings{wu2021neural,
   title = {Neural architecture search as sparse supernet},
   author = {Wu, Yan and Liu, Aoming and Huang, Zhiwu and Zhang, Siwei and Van Gool, Luc},
   booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
    volume={35},
   number={12},
  pages={10379--10387},
   year = {2021}
}</code></pre>
    </div>
    <p></p>
    <p>We model the NAS problem as a sparse supernet using a new continuous architecture representation
      with a mixture of sparsity constraints.</p>
    </div>
  </div>

      <hr>



      <div class="container2">
    <div class="item image2">
      <img src='imgs_main/facereenact.jpg' width=180; height="auto">
    </div>
    <div class="item text2">
      <a href="https://arxiv.org/abs/1908.03251">
        <papertitle>One-shot Face Reenactment</papertitle>
      </a>
      <br>
      <authorname-nolink>Yunxuan Zhang</authorname-nolink>,
      <strong>Siwei Zhang</strong>,
      <authorname-nolink>Yue He</authorname-nolink>,
      <a href=https://scholar.google.com/citations?user=F5rVlz0AAAAJ&hl=en><authorname>Cheng Li</authorname></a>,
      <a href=https://www.mmlab-ntu.com/person/ccloy/><authorname>Chen Change Loy</authorname></a>,
      <a href=https://liuziwei7.github.io/><authorname>Ziwei Liu</authorname></a>
      <br>
      <em>BMVC</em>, 2019 <font color="red"><strong>Spotlight Presentation</strong></font>
      <br>
        <div class="box">
             <a href="https://github.com/bj80heyue/One_Shot_Face_Reenactment"><i class="fa fa-github"></i> Code</a>
        </div>
        <div class="box">
            <a href="https://arxiv.org/abs/1908.03251"><i class="fa fa-file-pdf-o"></i> arXiv</a>
        </div>

<!--      <a href="https://github.com/bj80heyue/One_Shot_Face_Reenactment">Code</a> /-->
<!--      <a href="https://arxiv.org/abs/1908.03251">arXiv</a> /-->
      <button id="bib_button" class="box", onclick="showBib('facereenact_bib')"><i class="fa fa-quote-left"></i> BibTex</button>
      <div id='facereenact_bib' hidden>
      <pre><code>@inproceedings{zhang2019one,
   title = {One-shot Face Reenactment},
   author = {Zhang, Yunxuan and Zhang, Siwei and He, Yue and Li, Cheng and Loy, Chen Change and Liu, Ziwei},
   booktitle = {BMVC},
   month = September,
   year = {2019}
}</code></pre>
      </div>

      <p></p>
      <p>We propose a novel one-shot face reenactment learning system, that is able to disentangle and
        compose appearance and shape information for effective modeling.</p>
</div>
    </div>



        <div class="item text2">
            <p>
              <heading>Academic Services</heading>
            </p>
             <p>
            <ul>
                <li>Main organizer of <a href="https://sites.google.com/view/3d-humans-cvpr2024"> Humans workshop </a> at CVPR 2024.</li>
                <li>Main organizer of <a href="https://sites.google.com/view/egocentric-hand-body-activity"> Human Body, Hands, and Activities from Egocentric and Multi-view Cameras workshop </a> and EgoBody benchmark at ECCV 2022.</li>
                <li>Serve as reviewer for ICCV, CVPR, ECCV, 3DV, AAAI, and SIGGRAPH.</li>
            </ul>
            </p>
        </div>

        <div class="item text2">
            <p>
              <heading>Awards</heading>
            </p>
            <p>
            <ul>
                <li> <a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/2023-europe">Qualcomm Innovative Fellowship Europe 2023 </a>
            </ul>
            </p>
        </div>


        </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;">
                   Template adapted from <a href="https://jonbarron.info/">Jon Barron</a>'s and <a href="https://qianlim.github.io/">Qianli Ma</a>'s websites.
                </p>
              </td>
            </tr>
          </tbody></table>
      <script type="text/javascript" src="show_bib.js"></script>
    </div>
<br>
</body>
</html>
